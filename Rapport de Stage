# Lundi 16 juin 2025

## Ce jour, j’ai appris ce qu’étaient :

- les injections de prompts,
- un LLM,
- les mesures de protection mises en place par les modèles d’IA,
- et comment les contourner.

J’ai également créé mon compte GitHub.

J’ai découvert que les attaques par injection de prompts représentent à ce jour le plus grand risque pour l’IA.  
Elles permettent :

- d’obtenir des informations sensibles,  
- de générer de la désinformation,  
- et globalement, de changer le comportement de l’IA pour l’utiliser de manière dangereuse.
