# Lundi 16 juin 2025

## Ce jour, j’ai appris ce qu’étaient :

- les injections de prompts,
- un LLM,
- les mesures de protection mises en place par les modèles d’IA,
- et comment les contourner.

J’ai également créé mon compte GitHub.

J’ai découvert que les attaques par injection de prompts représentent à ce jour le plus grand risque pour l’IA.  
Elles permettent :

- d’obtenir des informations sensibles,  
- de générer de la désinformation,  
- et globalement, de changer le comportement de l’IA pour l’utiliser de manière dangereuse.

## Mardi 17 juin 2025

Ce jour, j’ai appris à installer un **LLM en local via LM Studios**.  
Grâce à cette installation, le modèle d’IA n’est plus restreint par son *system prompt* (liste de règles strictes définies par les développeurs), ce qui veut dire que l’utilisateur n’est également plus restreint par les limites établies.  
Par exemple, avec **ChatGPT-4o**, l’utilisateur n’aura plus la limite de messages comprise avec le plan gratuit disponible sur internet — il aura un **nombre illimité de requêtes**.

J’ai également appris à utiliser **A-Frame** afin de créer une scène avec des objets 3D.

J’ai aussi commencé un projet intitulé :  
**« Assistant IA pour la recherche de PFMP »**

